import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

import pandas as pd
import numpy as np
import json
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

# ==============================================================================
# BURGLARY PREDICTION MODEL - ABSOLUTELY NO DATA LEAKAGE
# ==============================================================================
# ULTRA-STRICT: Every preprocessing step fits on training data ONLY
# ==============================================================================

print("üè† BURGLARY PREDICTION MODEL - ZERO DATA LEAKAGE")
print("üö® ULTRA-STRICT MODE: NO SHORTCUTS")
print("=" * 60)

# 1) LOAD DATA - NO PREPROCESSING YET
# ==============================================================================
print("üìä Loading raw data...")

df = pd.read_csv(
    r"C:/Users/20232553/Downloads/burglaries_with_accom_and_hours_props.csv",
    usecols=['LSOA code','Ward code','Date','Burglary Count','covid_period',
             'LSOA Area Size (HA)','Overall Ranking - IMD','Housing rank',
             'Health rank','Living environment rank','Education rank',
             'Income rank','Employment rank',
             # NEW: Accommodation type proportions
             'prop_Detached','prop_Semi_detached','prop_Terraced',
             'prop_Purpose_built_flat','prop_Flat_converted_shared',
             'prop_Flat_commercial_building','prop_Caravan_other',
             # NEW: Hours worked proportions
             'prop_hrs_15_or_less','prop_hrs_16_30','prop_hrs_31_48','prop_hrs_49_more'],
    dtype={'LSOA code':'str','Ward code':'str','Burglary Count':'int16',
           'covid_period':'int8','LSOA Area Size (HA)':'float32',
           'Overall Ranking - IMD':'float32','Housing rank':'float32',
           'Health rank':'float32','Living environment rank':'float32',
           'Education rank':'float32','Income rank':'float32','Employment rank':'float32',
           # NEW: Accommodation and hours worked dtypes
           'prop_Detached':'float32','prop_Semi_detached':'float32','prop_Terraced':'float32',
           'prop_Purpose_built_flat':'float32','prop_Flat_converted_shared':'float32',
           'prop_Flat_commercial_building':'float32','prop_Caravan_other':'float32',
           'prop_hrs_15_or_less':'float32','prop_hrs_16_30':'float32',
           'prop_hrs_31_48':'float32','prop_hrs_49_more':'float32'},
    parse_dates=['Date']
)

# Basic renaming only
df.rename(columns={
    'LSOA code':'lsoa_code','Ward code':'ward_code',
    'Burglary Count':'burglary_count','Date':'date',
    'LSOA Area Size (HA)':'area_ha','Overall Ranking - IMD':'imd_overall',
    'Housing rank':'imd_housing','Health rank':'imd_health',
    'Living environment rank':'imd_living_env','Education rank':'imd_education',
    'Income rank':'imd_income','Employment rank':'imd_employment'
    # NEW: Keep accommodation and hours worked variable names as they are (already clean)
}, inplace=True)

df.sort_values(['lsoa_code','date'], inplace=True)
df.reset_index(drop=True, inplace=True)

print(f"‚úÖ Raw data loaded: {df.shape[0]:,} rows")

# 2) SPLIT IMMEDIATELY - BEFORE ANY FEATURE ENGINEERING
# ==============================================================================
print("\nüî™ SPLITTING DATA FIRST - NO PREPROCESSING APPLIED YET")
print("-" * 60)

# Split by date at 70% mark
split_date = df['date'].quantile(0.7)
print(f"Split date: {split_date.strftime('%Y-%m-%d')}")

# Create train/test masks
train_mask = df['date'] <= split_date
test_mask = df['date'] > split_date

# Split the raw data
df_train = df[train_mask].copy()
df_test = df[test_mask].copy()

print(f"Training: {df_train['date'].min().strftime('%Y-%m-%d')} to {df_train['date'].max().strftime('%Y-%m-%d')}")
print(f"Testing:  {df_test['date'].min().strftime('%Y-%m-%d')} to {df_test['date'].max().strftime('%Y-%m-%d')}")
print(f"Train size: {len(df_train):,}")
print(f"Test size:  {len(df_test):,}")

# Clear original dataframe to prevent accidental use
del df
print("‚úÖ Original dataframe deleted to prevent leakage")

# 3) TEMPORAL FEATURES - SAFE (DATE-BASED ONLY)
# ==============================================================================
print("\nüìÖ CREATING TEMPORAL FEATURES (SAFE)")
print("-" * 50)

def add_temporal_features(df_input):
    """Add temporal features based only on date - completely safe"""
    df_out = df_input.copy()
    
    # Basic time components
    df_out['year'] = df_out['date'].dt.year
    df_out['month'] = df_out['date'].dt.month
    df_out['quarter'] = df_out['date'].dt.quarter
    
    # Seasonality (safe - based only on month)
    df_out['sin_month'] = np.sin(2 * np.pi * df_out['month'] / 12)
    df_out['cos_month'] = np.cos(2 * np.pi * df_out['month'] / 12)
    df_out['sin_quarter'] = np.sin(2 * np.pi * df_out['quarter'] / 4)
    df_out['cos_quarter'] = np.cos(2 * np.pi * df_out['quarter'] / 4)
    
    # Season indicators
    df_out['is_winter'] = (df_out['month'].isin([12,1,2])).astype(int)
    df_out['is_summer'] = (df_out['month'].isin([6,7,8])).astype(int)
    
    # COVID dummy (uses existing column)
    df_out['covid_dummy'] = df_out['covid_period'].astype(int)
    
    return df_out

df_train = add_temporal_features(df_train)
df_test = add_temporal_features(df_test)

print("‚úÖ Temporal features added safely")

# 4) LAG FEATURES - MOST IMPORTANT, NO LEAKAGE POSSIBLE
# ==============================================================================
print("\n‚è∞ CREATING LAG FEATURES (HISTORICAL ONLY)")
print("-" * 50)

def add_lag_features(df_input):
    """Add lag features using only historical data"""
    df_out = df_input.copy()
    
    # Sort to ensure proper lag calculation
    df_out = df_out.sort_values(['lsoa_code', 'date']).reset_index(drop=True)
    
    # Create lags by LSOA
    grp = df_out.groupby('lsoa_code')['burglary_count']
    
    # Core lags
    for lag in [1, 2, 3, 6, 12]:
        df_out[f'lag_{lag}m'] = grp.shift(lag)
    
    # Seasonal lag
    df_out['lag_12m_seasonal'] = grp.shift(12)
    
    # Year-over-year change (critical feature)
    df_out['yoy_change'] = (df_out['lag_1m'] - df_out['lag_12m_seasonal']) / (df_out['lag_12m_seasonal'] + 1e-6)
    
    # Simple rolling means (shifted to avoid leakage)
    df_out['roll3_mean'] = grp.shift(1).rolling(3).mean()
    df_out['roll6_mean'] = grp.shift(1).rolling(6).mean()
    
    return df_out

df_train = add_lag_features(df_train)
df_test = add_lag_features(df_test)

print("‚úÖ Lag features added")

# 5) IMD FEATURES - FIT ON TRAIN ONLY, NO EXCEPTIONS
# ==============================================================================
print("\nüèòÔ∏è IMD FEATURES - STRICT TRAIN-ONLY FITTING")
print("-" * 50)

imd_cols = ['imd_overall','imd_housing','imd_health','imd_living_env',
            'imd_education','imd_income','imd_employment']

# Fit imputer ONLY on training data
print("Fitting IMD imputer on TRAINING data only...")
imd_imputer = SimpleImputer(strategy='median')
df_train[imd_cols] = imd_imputer.fit_transform(df_train[imd_cols])
print("Applying fitted imputer to TEST data...")
df_test[imd_cols] = imd_imputer.transform(df_test[imd_cols])

# Simple interactions (safe)
def add_imd_interactions(df_input):
    df_out = df_input.copy()
    df_out['income_x_education'] = df_out['imd_income'] * df_out['imd_education']
    df_out['covid_x_income'] = df_out['covid_dummy'] * df_out['imd_income']
    return df_out

df_train = add_imd_interactions(df_train)
df_test = add_imd_interactions(df_test)

print("‚úÖ IMD features processed with NO leakage")

# 6) NEW: ACCOMMODATION & HOURS FEATURES - FIT ON TRAIN ONLY
# ==============================================================================
print("\nüè† ACCOMMODATION & HOURS FEATURES - STRICT TRAIN-ONLY FITTING")
print("-" * 50)

# Define the new feature columns
accommodation_cols = ['prop_Detached','prop_Semi_detached','prop_Terraced',
                     'prop_Purpose_built_flat','prop_Flat_converted_shared',
                     'prop_Flat_commercial_building','prop_Caravan_other']

hours_cols = ['prop_hrs_15_or_less','prop_hrs_16_30','prop_hrs_31_48','prop_hrs_49_more']

# Fit imputer ONLY on training data for accommodation features
print("Fitting accommodation imputer on TRAINING data only...")
accom_imputer = SimpleImputer(strategy='median')
df_train[accommodation_cols] = accom_imputer.fit_transform(df_train[accommodation_cols])
print("Applying fitted accommodation imputer to TEST data...")
df_test[accommodation_cols] = accom_imputer.transform(df_test[accommodation_cols])

# Fit imputer ONLY on training data for hours worked features
print("Fitting hours worked imputer on TRAINING data only...")
hours_imputer = SimpleImputer(strategy='median')
df_train[hours_cols] = hours_imputer.fit_transform(df_train[hours_cols])
print("Applying fitted hours worked imputer to TEST data...")
df_test[hours_cols] = hours_imputer.transform(df_test[hours_cols])

# Create safe interactions with new features
def add_accommodation_hours_interactions(df_input):
    df_out = df_input.copy()
    
    # Accommodation interactions
    df_out['detached_x_covid'] = df_out['prop_Detached'] * df_out['covid_dummy']
    df_out['flat_total'] = (df_out['prop_Purpose_built_flat'] + 
                           df_out['prop_Flat_converted_shared'] + 
                           df_out['prop_Flat_commercial_building'])
    df_out['flat_total_x_income'] = df_out['flat_total'] * df_out['imd_income']
    
    # Hours worked interactions
    df_out['long_hours'] = df_out['prop_hrs_49_more']
    df_out['short_hours'] = df_out['prop_hrs_15_or_less']
    df_out['work_intensity'] = df_out['long_hours'] - df_out['short_hours']
    df_out['work_intensity_x_covid'] = df_out['work_intensity'] * df_out['covid_dummy']
    
    return df_out

df_train = add_accommodation_hours_interactions(df_train)
df_test = add_accommodation_hours_interactions(df_test)

print("‚úÖ Accommodation & Hours features processed with NO leakage")

# 7) WARD FEATURES - HISTORICAL ONLY
# ==============================================================================
print("\nüèòÔ∏è WARD FEATURES - HISTORICAL ONLY")
print("-" * 50)

def add_ward_features(df_input):
    """Add ward-level features using only historical data"""
    df_out = df_input.copy()
    df_out = df_out.sort_values(['ward_code', 'date']).reset_index(drop=True)
    
    wgrp = df_out.groupby('ward_code')['burglary_count']
    df_out['ward_lag1'] = wgrp.shift(1)
    df_out['ward_lag12'] = wgrp.shift(12)
    
    return df_out

df_train = add_ward_features(df_train)
df_test = add_ward_features(df_test)

print("‚úÖ Ward features added")

# 8) TARGET ENCODING - ULTRA-STRICT TRAIN-ONLY
# ==============================================================================
print("\nüéØ TARGET ENCODING - ULTRA-STRICT")
print("-" * 50)

def fit_target_encoding(df_train_input):
    """Fit target encoding on training data only, using LAG_1M as target"""
    
    # Use lag_1m as target to avoid direct leakage
    target_col = 'lag_1m'
    train_clean = df_train_input.dropna(subset=[target_col])
    
    if len(train_clean) == 0:
        print("ERROR: No valid lag_1m data for target encoding!")
        return {}
    
    print(f"Using {len(train_clean):,} training samples for target encoding")
    
    # Fit encoders
    encoders = {}
    
    def compute_target_encoding(series, target, smoothing=10):
        global_mean = target.mean()
        stats = pd.DataFrame({'cat': series, 'target': target}).groupby('cat')['target'].agg(['mean', 'count'])
        smoothed = (stats['mean'] * stats['count'] + global_mean * smoothing) / (stats['count'] + smoothing)
        return smoothed.to_dict(), global_mean
    
    # Fit on training data only
    encoders['lsoa'], encoders['lsoa_global'] = compute_target_encoding(
        train_clean['lsoa_code'], train_clean[target_col])
    encoders['ward'], encoders['ward_global'] = compute_target_encoding(
        train_clean['ward_code'], train_clean[target_col])
    encoders['month'], encoders['month_global'] = compute_target_encoding(
        train_clean['month'], train_clean[target_col])
    
    return encoders

def apply_target_encoding(df_input, encoders):
    """Apply fitted target encoders"""
    df_out = df_input.copy()
    
    if not encoders:
        print("No encoders available - skipping target encoding")
        return df_out
    
    df_out['lsoa_target_enc'] = df_out['lsoa_code'].map(encoders['lsoa']).fillna(encoders['lsoa_global'])
    df_out['ward_target_enc'] = df_out['ward_code'].map(encoders['ward']).fillna(encoders['ward_global'])
    df_out['month_target_enc'] = df_out['month'].map(encoders['month']).fillna(encoders['month_global'])
    
    return df_out

# Fit encoders on training data only
target_encoders = fit_target_encoding(df_train)

# Apply to both sets
df_train = apply_target_encoding(df_train, target_encoders)
df_test = apply_target_encoding(df_test, target_encoders)

print("‚úÖ Target encoding: FIT on train, APPLIED to both")

# 9) PREPARE FEATURES - ENHANCED SET WITH NEW FEATURES
# ==============================================================================
print("\nüîç FEATURE PREPARATION - ENHANCED SAFE SET")
print("-" * 50)

# Enhanced feature set including new accommodation and hours worked features
feature_columns = [
    # Temporal (safe)
    'year', 'month', 'quarter', 'sin_month', 'cos_month', 'sin_quarter', 'cos_quarter',
    'is_winter', 'is_summer', 'covid_dummy',
    
    # Lags (safe - historical only)
    'lag_1m', 'lag_2m', 'lag_3m', 'lag_6m', 'lag_12m', 'lag_12m_seasonal',
    'yoy_change', 'roll3_mean', 'roll6_mean',
    
    # Ward lags (safe)
    'ward_lag1', 'ward_lag12',
    
    # IMD (preprocessed safely)
    'imd_overall', 'imd_housing', 'imd_health', 'imd_living_env',
    'imd_education', 'imd_income', 'imd_employment',
    'income_x_education', 'covid_x_income',
    
    # Area (raw)
    'area_ha',
    
    # NEW: Accommodation types (preprocessed safely)
    'prop_Detached', 'prop_Semi_detached', 'prop_Terraced',
    'prop_Purpose_built_flat', 'prop_Flat_converted_shared',
    'prop_Flat_commercial_building', 'prop_Caravan_other',
    
    # NEW: Hours worked (preprocessed safely)
    'prop_hrs_15_or_less', 'prop_hrs_16_30', 'prop_hrs_31_48', 'prop_hrs_49_more',
    
    # NEW: Accommodation & Hours interactions (safe)
    'detached_x_covid', 'flat_total', 'flat_total_x_income',
    'long_hours', 'short_hours', 'work_intensity', 'work_intensity_x_covid',
    
    # Target encoding (fitted on train)
    'lsoa_target_enc', 'ward_target_enc', 'month_target_enc'
]

# Check which features actually exist
available_features = []
for col in feature_columns:
    if col in df_train.columns and col in df_test.columns:
        available_features.append(col)
    else:
        print(f"  Missing feature: {col}")

print(f"‚úÖ Available features: {len(available_features)}")

# 10) CLEAN DATA - REMOVE ROWS WITH MISSING CRITICAL FEATURES
# ==============================================================================
print("\nüßπ CLEANING DATA - REMOVE MISSING CRITICAL FEATURES")
print("-" * 50)

# Critical features that must not be missing
critical_features = ['lag_1m', 'lag_2m', 'lag_3m', 'roll3_mean']

print(f"Before cleaning - Train: {len(df_train):,}, Test: {len(df_test):,}")

# Remove rows with missing critical features
df_train_clean = df_train.dropna(subset=critical_features).copy()
df_test_clean = df_test.dropna(subset=critical_features).copy()

print(f"After cleaning  - Train: {len(df_train_clean):,}, Test: {len(df_test_clean):,}")

# Final feature matrix preparation
X_train = df_train_clean[available_features].copy()
X_test = df_test_clean[available_features].copy()
y_train = df_train_clean['burglary_count'].copy()
y_test = df_test_clean['burglary_count'].copy()

# Final imputation - fit on training only
print("Final imputation - fitting on TRAINING data only...")
final_imputer = SimpleImputer(strategy='median')
X_train_final = pd.DataFrame(
    final_imputer.fit_transform(X_train), 
    columns=available_features, 
    index=X_train.index
)
X_test_final = pd.DataFrame(
    final_imputer.transform(X_test), 
    columns=available_features, 
    index=X_test.index
)

print(f"‚úÖ Final matrices prepared")
print(f"   Training: {X_train_final.shape}")
print(f"   Test: {X_test_final.shape}")

# 11) MODEL TRAINING - SIMPLE AND CLEAN
# ==============================================================================
print("\nü§ñ MODEL TRAINING")
print("-" * 50)

# Simple parameters to avoid overfitting
params = {
    'n_estimators': 300,  # Reduced to prevent overfitting
    'max_depth': 4,       # Shallow trees
    'learning_rate': 0.05, # Slower learning
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_alpha': 1.0,     # More regularization
    'reg_lambda': 1.0,
    'objective': 'reg:squarederror',
    'random_state': 42,
    'n_jobs': -1
}

# Cross-validation
print("Cross-validation...")
tscv = TimeSeriesSplit(n_splits=3)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_final), 1):
    model = xgb.XGBRegressor(**params)
    model.fit(X_train_final.iloc[train_idx], y_train.iloc[train_idx])
    val_preds = model.predict(X_train_final.iloc[val_idx])
    fold_r2 = r2_score(y_train.iloc[val_idx], val_preds)
    cv_scores.append(fold_r2)
    print(f"  Fold {fold} R¬≤: {fold_r2:.4f}")

cv_mean = np.mean(cv_scores)
cv_std = np.std(cv_scores)
print(f"‚úÖ CV R¬≤: {cv_mean:.4f} ¬± {cv_std:.4f}")

# Final model
print("Training final model...")
final_model = xgb.XGBRegressor(**params)
final_model.fit(X_train_final, y_train)

# Predictions
test_predictions = final_model.predict(X_test_final)
df_test_clean['pred'] = test_predictions

# 12) WARD-LEVEL ANALYSIS
# ==============================================================================
print("\n" + "="*60)
print("üèòÔ∏è WARD-LEVEL ANALYSIS")
print("="*60)

# Aggregate to ward level
ward_agg = df_test_clean.groupby(['ward_code', 'date']).agg({
    'burglary_count': 'sum',
    'pred': 'sum'
}).reset_index()

# Overall ward performance
ward_r2 = r2_score(ward_agg['burglary_count'], ward_agg['pred'])
ward_mae = mean_absolute_error(ward_agg['burglary_count'], ward_agg['pred'])
ward_rmse = np.sqrt(mean_squared_error(ward_agg['burglary_count'], ward_agg['pred']))

print(f"üìä OVERALL WARD PERFORMANCE:")
print(f"   R¬≤:   {ward_r2:.4f}")
print(f"   MAE:  {ward_mae:.2f}")
print(f"   RMSE: {ward_rmse:.2f}")

# Find meaningful ward for visualization
ward_totals = ward_agg.groupby('ward_code')['burglary_count'].sum()
wards_with_crime = ward_totals[ward_totals >= 20].index  # At least 20 total crimes

if len(wards_with_crime) > 0:
    # Find best performing ward among those with crime
    best_ward = None
    best_r2 = -1
    
    for ward in wards_with_crime:
        ward_data = ward_agg[ward_agg['ward_code'] == ward]
        if len(ward_data) >= 12:
            ward_r2_individual = r2_score(ward_data['burglary_count'], ward_data['pred'])
            if ward_r2_individual > best_r2:
                best_r2 = ward_r2_individual
                best_ward = ward
    
    if best_ward:
        print(f"üèÜ Best ward: {best_ward} (R¬≤: {best_r2:.4f})")
        
        # Visualization
        ward_data = ward_agg[ward_agg['ward_code'] == best_ward].sort_values('date')
        
        plt.figure(figsize=(14, 8))
        
        plt.subplot(2, 1, 1)
        plt.plot(ward_data['date'], ward_data['burglary_count'], 'o-', 
                color='blue', label='Actual', alpha=0.7)
        plt.plot(ward_data['date'], ward_data['pred'], 's-', 
                color='red', label='Predicted', alpha=0.7)
        plt.title(f'Ward {best_ward} Performance (R¬≤: {best_r2:.3f})')
        plt.ylabel('Monthly Burglary Count')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 1, 2)
        residuals = ward_data['burglary_count'] - ward_data['pred']
        plt.plot(ward_data['date'], residuals, 'o-', color='orange', alpha=0.7)
        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        plt.title('Residuals')
        plt.ylabel('Actual - Predicted')
        plt.xlabel('Date')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# Feature importance
importance_df = pd.DataFrame({
    'feature': X_train_final.columns,
    'importance': final_model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nüîç TOP 15 FEATURES:")
for i, (_, row) in enumerate(importance_df.head(15).iterrows(), 1):
    print(f"{i:2d}. {row['feature']:<25} {row['importance']:.4f}")

# Analyze new feature performance
print(f"\nüè† NEW ACCOMMODATION FEATURES IMPORTANCE:")
accom_features = [f for f in importance_df['feature'] if any(x in f for x in ['prop_Detached', 'prop_Semi', 'prop_Terraced', 'prop_Purpose', 'prop_Flat', 'prop_Caravan', 'flat_total', 'detached_x'])]
for feat in accom_features:
    imp = importance_df[importance_df['feature'] == feat]['importance'].iloc[0]
    rank = importance_df[importance_df['feature'] == feat].index[0] + 1
    print(f"  {feat:<25} {imp:.4f} (rank #{rank})")

print(f"\n‚è∞ NEW HOURS WORKED FEATURES IMPORTANCE:")
hours_features = [f for f in importance_df['feature'] if any(x in f for x in ['prop_hrs', 'long_hours', 'short_hours', 'work_intensity'])]
for feat in hours_features:
    imp = importance_df[importance_df['feature'] == feat]['importance'].iloc[0]
    rank = importance_df[importance_df['feature'] == feat].index[0] + 1
    print(f"  {feat:<25} {imp:.4f} (rank #{rank})")

# Final summary
covid_count = df_test_clean[df_test_clean['covid_period']==1].shape[0]
total_count = df_test_clean.shape[0]

print(f"\n" + "="*60)
print("üìà ENHANCED RESULTS - NO DATA LEAKAGE GUARANTEED")
print("="*60)
print(f"üèòÔ∏è Ward R¬≤:        {ward_r2:.4f}")
print(f"üìä Ward MAE:       {ward_mae:.2f}")
print(f"üîÑ CV R¬≤:          {cv_mean:.4f} ¬± {cv_std:.4f}")
print(f"üéØ Total Features: {len(available_features)}")
print(f"üè† Accommodation:  {len(accom_features)} features")
print(f"‚è∞ Hours Worked:   {len(hours_features)} features")
print(f"üìö Train samples:  {len(X_train_final):,}")
print(f"üß™ Test samples:   {len(X_test_final):,}")
print(f"ü¶† COVID in test:  {covid_count:,} / {total_count:,} ({100*covid_count/total_count:.1f}%)")
print(f"")
print(f"üõ°Ô∏è DATA LEAKAGE PREVENTION MEASURES:")
print(f"   ‚úÖ Temporal split FIRST")
print(f"   ‚úÖ All preprocessing fit on train only")
print(f"   ‚úÖ Target encoding fit on train only")
print(f"   ‚úÖ Accommodation features fit on train only")
print(f"   ‚úÖ Hours worked features fit on train only")
print(f"   ‚úÖ All imputation fit on train only")
print(f"   ‚úÖ Only historical lag features used")
print(f"   ‚úÖ Conservative model parameters")
print("="*60)

# Save enhanced results
ward_agg.to_csv('ward_predictions_enhanced_no_leakage.csv', index=False)
importance_df.to_csv('feature_importance_enhanced_no_leakage.csv', index=False)

print(f"\nüéâ ENHANCED ANALYSIS COMPLETE - ZERO DATA LEAKAGE")
print(f"üìä Ward R¬≤: {ward_r2:.4f} (realistic performance)")
print(f"üíæ Enhanced results saved to CSV files")